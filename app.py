import streamlit as st
import os
import pandas as pd
import numpy as np
from typing import List

# Hugging Face KÃ¼tÃ¼phaneleri
from huggingface_hub import InferenceClient
from sentence_transformers import SentenceTransformer
from datasets import load_dataset

# ------------------------------------------------
# Hugging Face API Token (Opsiyonel ama Ã–nerilir)
# ------------------------------------------------
# HF_TOKEN = os.environ.get("HF_TOKEN") # Streamlit Secrets'a HF_TOKEN ekleyebilirsiniz (daha yÃ¼ksek limitler iÃ§in)
# if not HF_TOKEN:
#     st.warning("Hugging Face API Token bulunamadÄ±. Ãœcretsiz limitlerle devam ediliyor.")

# Model isimleri
embedding_model_name = 'all-MiniLM-L6-v2'
# LLM: Hugging Face Inference API Ã¼zerinden popÃ¼ler bir model
llm_model_name = "mistralai/Mistral-7B-Instruct-v0.1" # Veya daha kÃ¼Ã§Ã¼k bir model: "HuggingFaceH4/zephyr-7b-beta"

# Hugging Face Inference Client'Ä± baÅŸlatma
try:
    # HF_TOKEN varsa kullanÄ±lÄ±r, yoksa Ã¼cretsiz limitlerle Ã§alÄ±ÅŸÄ±r
    hf_client = InferenceClient(model=llm_model_name) #, token=HF_TOKEN) 
except Exception as e:
    st.error(f"Hugging Face Inference Client baÅŸlatÄ±lÄ±rken hata: {e}")
    st.stop()


# ------------------------------------------------
# Sentence Transformer Modelini YÃ¼kleme (Cache ile)
# ------------------------------------------------
@st.cache_resource(show_spinner="Embedding modeli yÃ¼kleniyor...")
def load_embedding_model():
    return SentenceTransformer(embedding_model_name)

# ------------------------------------------------
# Tarifleri yÃ¼kleme (DeÄŸiÅŸiklik yok)
# ------------------------------------------------
# app.py dosyasÄ±ndaki load_recipes fonksiyonunu bulun:

@st.cache_data(show_spinner="Tarifler yÃ¼kleniyor...")
def load_recipes() -> list[str]:
    # ğŸ›‘ DEÄÄ°ÅÄ°KLÄ°K BURADA: 50 yerine 200 tarif yÃ¼kle
    ds = load_dataset("Hieu-Pham/kaggle_food_recipes", split="train[:200]") 
    # ... (fonksiyonun geri kalanÄ± aynÄ±) ...
    recipes = []
    for item in ds:
        title = item.get("Title", "")
        ingredients = item.get("Ingredients", [])
        if isinstance(ingredients, list):
            ingredients = ", ".join(ingredients)
        instructions = item.get("Instructions", "")
        full_recipe = f"TARÄ°F ADI: {title}\nMALZEMELER: {ingredients}\nADIMLAR: {instructions}"
        recipes.append(full_recipe)
    return recipes

# ------------------------------------------------
# Veri ve Embedding Cache (Sentence Transformer ile)
# ------------------------------------------------
@st.cache_data(show_spinner="Veriler ve embeddingler hazÄ±rlanÄ±yor...")
def load_data_and_embeddings(_embedding_model):
    recipe_docs = load_recipes()
    doc_ids = [f"doc_{i}" for i in range(len(recipe_docs))]
    try:
        embeds = _embedding_model.encode(recipe_docs, show_progress_bar=False).tolist()
    except Exception as e:
        st.error(f"Embedding oluÅŸturulurken hata: {str(e)}")
        st.stop()
    return recipe_docs, doc_ids, np.array(embeds)


# âœ… KosinÃ¼s benzerliÄŸi (DeÄŸiÅŸiklik yok)
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return np.dot(a, b) / (norm_a * norm_b)

# ------------------------------------------------
# UI
# ------------------------------------------------
st.set_page_config(page_title="Yemek Tarifleri Chatbotu", layout="wide")
st.title("ğŸ½ï¸ GenAI Yemek Tarifleri Chatbotu ğŸ¥—")
st.divider()

# Embedding modelini yÃ¼kle
embedding_model = load_embedding_model()
# Veri ve embeddingleri yÃ¼kle
docs, ids, embeddings = load_data_and_embeddings(embedding_model)

st.caption(f"Veri tabanÄ±mÄ±zda {len(docs)} tarif bulunmaktadÄ±r. (LLM: {llm_model_name} | Embedding: {embedding_model_name})")

if "history" not in st.session_state:
    st.session_state.history = []

query = st.chat_input("Ne piÅŸirmek istersin? (Ã¶rn: IspanaklÄ± bir ÅŸey)")

# app.py dosyasÄ±ndaki if query: bloÄŸunun iÃ§indeki try...except bÃ¶lÃ¼mÃ¼

if query:
    st.session_state.history.append({"role": "user", "content": query})

    with st.spinner("Tarif aranÄ±yor ve yanÄ±t oluÅŸturuluyor..."):
        try:
            # ... (Embedding ve Arama kodlarÄ± aynÄ± kalÄ±r) ...
            
            # 4. Prompt oluÅŸturma (DeÄŸiÅŸiklik yok)
            prompt = f"""<s>[INST] AÅŸaÄŸÄ±daki baÄŸlamda sana verilen yemek tariflerini kullanarak, kullanÄ±cÄ±nÄ±n sorusuna detaylÄ± ve yardÄ±mcÄ± bir ÅŸekilde yanÄ±t ver. 
EÄŸer baÄŸlamda uygun tarif bulamazsan, kibarca sadece "ÃœzgÃ¼nÃ¼m, veri tabanÄ±mda bu isteÄŸe uygun bir tarif bulamadÄ±m." diye yanÄ±tla.

BAÄLAM:
{context}

SORU: {query} [/INST]
YANIT:"""

            # 5. Hugging Face Inference API'sine gÃ¶nderme
            response = hf_client.text_generation(
                prompt,
                max_new_tokens=250, 
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.1
            )
            
            llm_response = response.strip() 

            st.session_state.history.append({"role": "assistant", "content": llm_response, "sources": source_names})

        # ğŸ›‘ HATA YAKALAMAYI DETAYLANDIRDIK ğŸ›‘
        except Exception as e:
            import traceback # Traceback'i yazdÄ±rmak iÃ§in import et
            tb_str = traceback.format_exc() # HatanÄ±n tam traceback'ini al
            
            # Hem arayÃ¼ze hem de loglara detaylÄ± hata yazdÄ±r
            error_msg = f"RAG/API HatasÄ± OluÅŸtu!\nDetaylar:\n{str(e)}\n\nTraceback:\n{tb_str}"
            st.error(error_msg) # ArayÃ¼zde gÃ¶ster
            print(error_msg)    # Streamlit loglarÄ±na yazdÄ±r
            
            # GeÃ§miÅŸe basit hata mesajÄ± ekle
            st.session_state.history.append({"role": "assistant", "content": f"ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu: {str(e)}", "sources": []})

# ... (GeÃ§miÅŸi gÃ¶sterme kodu aynÄ± kalÄ±r) ...

# GeÃ§miÅŸi gÃ¶sterme (DeÄŸiÅŸiklik yok)
for msg in st.session_state.history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and msg.get("sources"):
            st.markdown("---")
            st.markdown("**KullanÄ±lan Kaynak Tarifler:**")
            for name in set(msg.get("sources", [])):
                st.markdown(f"**-** *{name}*")
